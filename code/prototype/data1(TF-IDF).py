import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import sys
import random
import json

# --- 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))

# ì…ë ¥ íŒŒì¼ ê²½ë¡œ
INTERACTION_FILE = os.path.join(project_root, 'output', 'final_user_interactions.csv')
USER_PROFILE_FILE = os.path.join(project_root, 'output', 'final_user_profiles.json')
PAPER_KEYWORDS_FILE = os.path.join(project_root, 'output', 'paper_keywords_tfidf.json')
TITLE_FILE = os.path.join(project_root, 'subdataset', 'titleabs.tsv') # ì œëª© ë¡œë”©ìš©

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (Epoch ì¦ê°€)
EMBEDDING_DIM = 64   # ì°¨ì› ìˆ˜ë„ ì¡°ê¸ˆ ëŠ˜ë¦¼ (í‘œí˜„ë ¥ ì¦ëŒ€)
BATCH_SIZE = 1024    # GPU í™œìš©ì„ ìœ„í•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ í‚¤ì›€
LEARNING_RATE = 0.005
EPOCHS = 30          # í•™ìŠµ íšŸìˆ˜ ëŒ€í­ ì¦ê°€ (5 -> 30)
TOP_K = 5            

# ì¥ì¹˜ ì„¤ì • (CUDA ìš°ì„ )
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"í•™ìŠµ ì¥ì¹˜ ì„¤ì •: **{device}**")
if device.type == 'cuda':
    print(f"   - GPU Name: {torch.cuda.get_device_name(0)}")

print("\n1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")

# 1-1. ì¸í„°ë™ì…˜ ë°ì´í„°
if not os.path.exists(INTERACTION_FILE):
    sys.exit(f"ì˜¤ë¥˜: {INTERACTION_FILE} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
df = pd.read_csv(INTERACTION_FILE, dtype={'paper_id': str})

# 1-2. ë…¼ë¬¸ ì œëª© ë§¤í•‘ ë°ì´í„° ë¡œë“œ (titleabs.tsv)
print("   - ë…¼ë¬¸ ì œëª© ë°ì´í„° ë¡œë”© ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
try:
    # TSV ë¡œë“œ (í—¤ë” ì—†ìŒ: paper_id, title, abstract)
    df_titles = pd.read_csv(TITLE_FILE, sep='\t', header=None, usecols=[0, 1],
                            names=['paper_id', 'title'], dtype={'paper_id': str})
    # paper_id -> title ë”•ì…”ë„ˆë¦¬ ìƒì„±
    paper_title_map = dict(zip(df_titles['paper_id'], df_titles['title']))
    print(f"   - ì œëª© ë§¤í•‘ ì™„ë£Œ: {len(paper_title_map):,}ê°œ ë…¼ë¬¸")
except Exception as e:
    print(f"ì œëª© íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    paper_title_map = {}

# 1-3. ì•„ì´í…œ ID -> Paper ID ë§¤í•‘ (ì¶”ì²œ ê²°ê³¼ í•´ì„ìš©)
# item_idëŠ” 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤, paper_idëŠ” ì›ë³¸ ID
item_to_paper_id = df.set_index('item_id')['paper_id'].to_dict()

num_users = df['user_id'].max() + 1
num_items = df['item_id'].max() + 1
print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: ìœ ì € {num_users}ëª…, ì•„ì´í…œ {num_items}ê°œ, ì¸í„°ë™ì…˜ {len(df)}ê±´")

# í‚¤ì›Œë“œ ë¡œë“œ
with open(PAPER_KEYWORDS_FILE, 'r', encoding='utf-8') as f:
    paper_keywords = json.load(f)

# --- 2. ëª¨ë¸ ì •ì˜ (Matrix Factorization) ---
class MatrixFactorization(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim):
        super(MatrixFactorization, self).__init__()
        self.user_emb = nn.Embedding(num_users, embedding_dim)
        self.item_emb = nn.Embedding(num_items, embedding_dim)
        
        # Xavier Initialization (í•™ìŠµ ì´ˆê¸°í™” ê°œì„ )
        nn.init.xavier_uniform_(self.user_emb.weight)
        nn.init.xavier_uniform_(self.item_emb.weight)

    def forward(self, user, item):
        u = self.user_emb(user)
        i = self.item_emb(item)
        return (u * i).sum(1)

# --- 3. ë°ì´í„°ì…‹ ì •ì˜ ---
class InteractionDataset(Dataset):
    def __init__(self, user_ids, item_ids, num_items, neg_ratio=4):
        self.users = user_ids
        self.items = item_ids
        self.num_items = num_items
        self.neg_ratio = neg_ratio
        self.users_set = set(zip(user_ids, item_ids))

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        u = self.users[idx]
        i = self.items[idx]
        samples = [(u, i, 1.0)]
        for _ in range(self.neg_ratio):
            neg_item = random.randint(0, self.num_items - 1)
            while (u, neg_item) in self.users_set:
                neg_item = random.randint(0, self.num_items - 1)
            samples.append((u, neg_item, 0.0))
        return samples

# --- 4. ğŸ‹ï¸í•™ìŠµ (Training) ---
dataset = InteractionDataset(df['user_id'].values, df['item_id'].values, num_items)

def collate_fn(batch):
    users, items, labels = [], [], []
    for samples in batch:
        for u, i, l in samples:
            users.append(u)
            items.append(i)
            labels.append(l)
    # í…ì„œ ìƒì„± ë° GPU ì´ë™
    return torch.tensor(users), torch.tensor(items), torch.tensor(labels)

dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

model = MatrixFactorization(num_users, num_items, EMBEDDING_DIM).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print(f"\n--- í•™ìŠµ ì‹œì‘ (Epochs: {EPOCHS}) ---")
model.train()

for epoch in range(EPOCHS):
    total_loss = 0
    for user_batch, item_batch, label_batch in dataloader:
        # GPUë¡œ ì´ë™
        user_batch = user_batch.to(device)
        item_batch = item_batch.to(device)
        label_batch = label_batch.to(device)
        
        optimizer.zero_grad()
        predictions = model(user_batch, item_batch)
        loss = criterion(predictions, label_batch.float())
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    # 5 epochë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    if (epoch + 1) % 5 == 0 or epoch == 0:
        print(f"Epoch {epoch+1:02d}/{EPOCHS}, Loss: {total_loss/len(dataloader):.4f}")

# --- 5. ì¶”ì²œ ë° ê²°ê³¼ í™•ì¸ ---
print("\n--- ì¶”ì²œ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜ ---")

model.eval()

# í—¤ë¹„ ìœ ì € ì„ ì •
heavy_users = df['user_id'].value_counts()
heavy_users = heavy_users[heavy_users >= 5].index.tolist()
if not heavy_users: heavy_users = df['user_id'].unique().tolist()
target_user_id = random.choice(heavy_users)

# ìœ ì € ì •ë³´ ë¡œë“œ
with open(USER_PROFILE_FILE, 'r', encoding='utf-8') as f:
    profiles = json.load(f)
    user_info = next((p for p in profiles if p['user_id'] == target_user_id), None)
    user_name = user_info['author_name'] if user_info else "Unknown"

# ì‹¤ì œ íˆìŠ¤í† ë¦¬
real_history_items = df[df['user_id'] == target_user_id]['item_id'].tolist()

# ì˜ˆì¸¡ ìˆ˜í–‰ (ì „ì²´ ì•„ì´í…œì— ëŒ€í•´)
all_items = torch.arange(num_items).to(device)
target_user_tensor = torch.tensor([target_user_id] * num_items).to(device)

with torch.no_grad():
    predictions = model(target_user_tensor, all_items)
    scores = torch.sigmoid(predictions).cpu().numpy()

# ì´ë¯¸ ë³¸ ê²ƒì€ ì œì™¸
scores[real_history_items] = -1
top_k_indices = np.argsort(scores)[::-1][:TOP_K]

# --- 6. ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥ ---
print(f"\n**Target User**: {user_name} (ID: {target_user_id})")
print(f"**ì‘ì„± ë…¼ë¬¸ ìˆ˜**: {len(real_history_items)}í¸")
print(f"**ì£¼ìš” ê´€ì‹¬ì‚¬ (Top Keywords)**: {user_info['top_keywords'] if user_info else 'N/A'}")

print(f"\n**[Top {TOP_K} ì¶”ì²œ ë…¼ë¬¸]**")
print("-" * 60)

for rank, item_idx in enumerate(top_k_indices, 1):
    # 1. Paper ID ì°¾ê¸°
    paper_id = str(item_to_paper_id.get(item_idx, "Unknown"))
    
    # 2. ì œëª© ì°¾ê¸°
    title = paper_title_map.get(paper_id, "Title Not Found")
    
    # 3. í‚¤ì›Œë“œ ì°¾ê¸°
    keywords = paper_keywords.get(paper_id, ["No Keywords"])
    
    # 4. ì ìˆ˜
    score = scores[item_idx]
    
    print(f"[{rank}ìœ„] (ìœ ì‚¬ë„: {score:.1%})")
    print(f"  **Title**: {title}")
    print(f"  **Keywords**: {', '.join(keywords)}")
    print("-" * 60)

print("\nì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ.")