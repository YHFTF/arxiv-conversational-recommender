import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
import pandas as pd
import numpy as np
import os
import sys
import json
from collections import Counter
from openai import OpenAI
from dotenv import load_dotenv
from torch.nn.functional import cosine_similarity # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€

# --- 0. í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY: 
    print("[Error] OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit(1)
client = OpenAI(api_key=API_KEY)
LLM_MODEL = "gpt-4o-mini"

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
LLM_RESULT_FILE = os.path.join(project_root, 'output', 'llm_extraction_results.json')
TITLE_FILE = os.path.join(project_root, 'subdataset', 'titleabs.tsv')
INTERACTION_FILE = os.path.join(project_root, 'output', 'final_user_interactions.csv')
AUTHOR_DATA_FILE = os.path.join(project_root, 'output', 'author_data_openalex.json')

# í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (data1(llmv2).pyì—ì„œ ìƒì„±ë¨)
MODEL_LOAD_PATH = os.path.join(project_root, 'output', 'content_aware_net_v2.pth')

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ê°’ê³¼ ë™ì¼í•´ì•¼ í•¨)
EMBEDDING_DIM = 64
TOP_K = 5

# --- [í™•ì¥ì„±] ë°ì´í„° 2/3 Reasoning ê´€ë ¨ ì„¤ì • ---
INCLUDE_REASONING_FEATURE = False # Data 2/3 êµ¬ì¶• ì™„ë£Œ ì‹œ Trueë¡œ ë³€ê²½
WEIGHT_DOMAIN = 2 # ë…¼ë¬¸ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
WEIGHT_TASK_METHOD = 1 # ë…¼ë¬¸ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
WEIGHT_REASONING = 1 # Reasoning í…ìŠ¤íŠ¸ì— ë¶€ì—¬í•  ë‚®ì€ ê°€ì¤‘ì¹˜

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[System] Device: {device}")


# --- 1. ëª¨ë¸ ì •ì˜ (ContentAwareNet) ---
# í•™ìŠµ ì‹œì™€ ë™ì¼í•œ êµ¬ì¡°ì—¬ì•¼ í•©ë‹ˆë‹¤.
class ContentAwareNet(nn.Module):
    def __init__(self, num_users, num_items, num_keywords, embedding_dim):
        super(ContentAwareNet, self).__init__()
        
        self.user_emb = nn.Embedding(num_users, embedding_dim)
        self.keyword_emb = nn.EmbeddingBag(num_keywords, embedding_dim, mode='mean', padding_idx=0)
        
        self.fc1 = nn.Linear(embedding_dim * 2, 128) 
        self.fc2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 1) 
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3) 
        
    def forward(self, user, item, keyword_ids):
        # ì´ í•¨ìˆ˜ëŠ” í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©ë˜ì§€ë§Œ, êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ ë³´ì¡´
        u_vec = self.user_emb(user)            
        k_vec = self.keyword_emb(keyword_ids)  
        
        x = torch.cat([u_vec, k_vec], dim=1)   
        
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        x = self.fc2(x)
        x = self.relu(x)
        
        logits = self.output(x)
        return logits.squeeze()

# --- 2. LLMì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìˆ˜ì •ë¨: ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ ìš”ì²­) ---
def extract_structured_keywords_from_prompt(prompt, include_reasoning=False):
    """ìì—°ì–´ í”„ë¡¬í”„íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    system_prompt = "You are an expert Research Analyst. Extract the core technical concepts from the user's research interest."
    
    reasoning_field = ""
    if include_reasoning:
        reasoning_field = ", \"reasoning\": \"...\""
        
    user_prompt = f"""
    Analyze the following research interest and extract 7 to 10 critical keywords. 
    Return ONLY a JSON object of the following format: {{ "keywords": [..] {reasoning_field} }}
    User Research Interest: "{prompt}"
    """
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            response_format={"type": "json_object"},
            temperature=0.0,
            max_tokens=150
        )
        # LLM ì¶œë ¥ì€ {'keywords': [..]} í˜•íƒœê°€ ë©ë‹ˆë‹¤.
        return json.loads(response.choices[0].message.content) 
    except Exception as e:
        print(f"[Error] LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None

# --- 3. í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ -> ì„ë² ë”© ë²¡í„° ìƒì„± (ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬) ---
def create_prompt_keyword_tensor(structured_kws, keyword_to_id_map):
    """
    í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œë¥¼ ëª¨ë¸ ì…ë ¥ í…ì„œë¡œ ë³€í™˜ (ë‹¨ìˆœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬).
    """
    kw_ids = []
    
    # 1. [ìˆ˜ì •] Prompt Keywords ì²˜ë¦¬ (ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸, ê°€ì¤‘ì¹˜ 1ë°°)
    # LLM ì¶œë ¥ì—ì„œ 'keywords' í•„ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    for kw in structured_kws.get('keywords', []): 
        kw_lower = kw.strip().lower()
        if kw_lower in keyword_to_id_map:
            kw_ids.append(keyword_to_id_map[kw_lower]) # 1ë°° ê°€ì¤‘ì¹˜ (default)

    # 2. [í™•ì¥ì„±] Reasoning í†µí•© (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if INCLUDE_REASONING_FEATURE:
        reasoning_text = structured_kws.get('reasoning', "")
        reasoning_words = [w.strip().lower() for w in reasoning_text.split() if w.isalpha()] 
        for word in reasoning_words:
            if word in keyword_to_id_map:
                kw_ids.extend([keyword_to_id_map[word]] * WEIGHT_REASONING)
                
    if not kw_ids: kw_ids = [0] 
    
    # ì—¬ê¸°ì„œ ë§Œë“¤ì–´ì§€ëŠ” ê²ƒì€ 'í•˜ë‚˜ì˜' í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    return torch.tensor([kw_ids], dtype=torch.long).to(device)


# --- 4. ì¶”ì²œ ì—”ì§„ (Cold-Start Mode - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì¢… ì ìš©) ---
def get_recommendations_for_prompt(model, prompt_kw_tensor, all_item_kws_tensor, num_items, top_k):
    """
    Cold-Start ì‹œ MLPì˜ í¸í–¥ì„ í”¼í•˜ê³ , í•™ìŠµëœ ì„ë² ë”©ì„ ì´ìš©í•œ ìˆœìˆ˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    num_total_items = num_items
    
    with torch.no_grad():
        # 1. Prompt Keyword Vector ìƒì„± (V_prompt)
        # model.keyword_embëŠ” í•™ìŠµëœ í‚¤ì›Œë“œ ì„ë² ë”© ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        v_prompt = model.keyword_emb(prompt_kw_tensor) 

        # 2. Item Keyword Vector ìƒì„± (V_item)
        v_item = model.keyword_emb(all_item_kws_tensor)
        
        # 3. [í•µì‹¬] ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        # Prompt Vectorë¥¼ Item ìˆ˜ë§Œí¼ ë³µì œí•˜ì—¬ Item Vectorì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
        v_prompt_repeated = v_prompt.repeat(num_total_items, 1)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸¡ì • (ì¶œë ¥ ë²”ìœ„: -1.0 ~ 1.0)
        preds = cosine_similarity(v_prompt_repeated, v_item).cpu().numpy()
        
        # ìŠ¤ì½”ì–´ ì •ê·œí™” (0.0% ~ 100.0%ë¡œ ë³€í™˜): (preds + 1) / 2
        # ìŒìˆ˜ ì ìˆ˜ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ 0.0 ~ 1.0 ë²”ìœ„ë¡œ ë³€í™˜ í›„ 100ì„ ê³±í•©ë‹ˆë‹¤.
        preds = (preds + 1) / 2 
        
    top_indices = np.argsort(preds)[::-1][:top_k]

    return top_indices, preds


# --- 5. ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ ë¡œë”© (Item Keyword Tensor ë¡œë”© ë¡œì§ í¬í•¨) ---
def load_all_data():
    if not os.path.exists(LLM_RESULT_FILE): sys.exit("[Error] LLM result file not found.")

    # 1. LLM ë°ì´í„° ë° í‚¤ì›Œë“œ ì‚¬ì „ ë¡œë”©
    with open(LLM_RESULT_FILE, 'r', encoding='utf-8') as f:
        llm_data = json.load(f)

    node_to_structured_keywords = {} 
    all_keywords = []
    
    for item in llm_data:
        node_idx = item['node_idx']
        
        # [ìˆ˜ì •] LLM ê²°ê³¼ì—ì„œ 'domain', 'task', 'method' ëŒ€ì‹  'keywords'ë¥¼ ì¶”ì¶œ
        # í•˜ì§€ë§Œ ë…¼ë¬¸ ìª½ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬ë¥¼ ìœ„í•´ ê¸°ì¡´ D/T/M êµ¬ì¡°ë¥¼ ìœ ì§€í•œë‹¤ê³  ê°€ì •í•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤.
        # ê¸°ì¡´ LLM ì¶”ì¶œ íŒŒì¼ì´ D/T/M êµ¬ì¡°ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
        domain_kws = [k.strip().lower() for k in item.get('domain', [])]
        task_kws = [k.strip().lower() for k in item.get('task', [])]
        method_kws = [k.strip().lower() for k in item.get('method', [])]
        
        node_to_structured_keywords[node_idx] = {'domain': domain_kws, 'task': task_kws, 'method': method_kws}
        
        all_keywords.extend(domain_kws)
        all_keywords.extend(task_kws)
        all_keywords.extend(method_kws)
        
    keyword_counts = Counter(all_keywords)
    unique_keywords = sorted(keyword_counts.keys())
    keyword_to_id = {kw: i+1 for i, kw in enumerate(unique_keywords)}
    NUM_KEYWORDS = len(keyword_to_id) + 1

    # 2. ë°ì´í„° ë§¤í•‘ (ëª¨ë¸ í¬ê¸° ì‚°ì •ìš©)
    df_full = pd.read_csv(INTERACTION_FILE)
    valid_nodes = set(node_to_structured_keywords.keys())
    df = df_full[df_full['item_id'].isin(valid_nodes)].copy()
    user_ids = df['user_id'].unique()
    item_ids = df['item_id'].unique()
    num_users = len(user_ids)
    num_items = len(item_ids)

    reverse_item_map = {new: original for new, original in enumerate(item_ids)}

    # 3. ëª¨ë“  ì•„ì´í…œì˜ í‚¤ì›Œë“œ í…ì„œ ì‚¬ì „ ê³„ì‚° (í•™ìŠµ ì‹œì˜ ê°€ì¤‘ì¹˜ ë¡œì§ ì¬í˜„)
    item_keyword_indices = {}
    for new_id, original_id in reverse_item_map.items():
        structured_kws = node_to_structured_keywords.get(original_id, {'domain': [], 'task': [], 'method': []})
        kw_ids = []
        # ë…¼ë¬¸ í‚¤ì›Œë“œëŠ” D/T/M ê°€ì¤‘ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        for kw in structured_kws['domain']:
            if kw in keyword_to_id: kw_ids.extend([keyword_to_id[kw]] * WEIGHT_DOMAIN)
        for kw in structured_kws['task'] + structured_kws['method']:
            if kw in keyword_to_id: kw_ids.extend([keyword_to_id[kw]] * WEIGHT_TASK_METHOD)
        if not kw_ids: kw_ids = [0]
        item_keyword_indices[new_id] = kw_ids
    
    all_kws_list = [torch.tensor(item_keyword_indices[i], dtype=torch.long) for i in range(num_items)]
    all_item_kws_tensor = pad_sequence(all_kws_list, batch_first=True, padding_value=0).to(device)


    # 4. ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”
    model = ContentAwareNet(num_users, num_items, NUM_KEYWORDS, EMBEDDING_DIM).to(device)
    if not os.path.exists(MODEL_LOAD_PATH): sys.exit(f"[Error] ëª¨ë¸ íŒŒì¼ {MODEL_LOAD_PATH}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”.")
    
    model.load_state_dict(torch.load(MODEL_LOAD_PATH, map_location=device))
    
    # Cold-Start ì¶”ì²œì„ ìœ„í•´ 0ë²ˆ ìœ ì € ì„ë² ë”©ì„ 0ë²¡í„°(ì¤‘ë¦½)ë¡œ ì„¤ì •
    if model.user_emb.weight.data.size(0) > 0:
        model.user_emb.weight.data[0].fill_(0.0) 

    model.eval()

    # 5. ì œëª© ë° ì €ì ì •ë³´ ë¡œë“œ
    df_titles = pd.read_csv(TITLE_FILE, sep='\t', header=None, usecols=[0, 1],
                            names=['paper_id', 'title'], dtype={'paper_id': str})
    paper_title_map = dict(zip(df_titles['paper_id'], df_titles['title']))
    
    try:
        with open(AUTHOR_DATA_FILE, 'r', encoding='utf-8') as f:
            author_raw = json.load(f)
        node_to_authors = {item['node_idx']: item['authors'] for item in author_raw}
    except: node_to_authors = {}

    # ìµœì¢… ë¦¬í„´ê°’ì— all_item_kws_tensor ì¶”ê°€
    return model, keyword_to_id, num_items, reverse_item_map, \
           node_to_structured_keywords, paper_title_map, node_to_authors, df_full, \
           all_item_kws_tensor


# --- 6. ë©”ì¸ ëŒ€í™”í˜• ë£¨í”„ ---
def main():
    print("="*60)
    print("    ğŸš€ LLM ê¸°ë°˜ ëŒ€í™”í˜• ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ (ContentAwareNet V2) ğŸš€")
    print("="*60)
    print("[System] ë°ì´í„° ë° ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    try:
        # load_all_data í˜¸ì¶œë¶€ ìˆ˜ì • (all_item_kws_tensor ë°›ê¸°)
        model, keyword_to_id, num_items, reverse_item_map, \
        node_to_structured_keywords, paper_title_map, node_to_authors, df_full, \
        all_item_kws_tensor = load_all_data()
        
        print(f"[System] ë¡œë”© ì™„ë£Œ. ì´ ë…¼ë¬¸ ìˆ˜: {num_items}, ì´ í‚¤ì›Œë“œ ìˆ˜: {len(keyword_to_id)}")

    except Exception as e:
        print(f"[Fatal Error] ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    print("\n[Start] ì¶”ì²œì„ ì›í•˜ëŠ” ì—°êµ¬ ë¶„ì•¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”. (Ctrl+Cë¡œ ì¢…ë£Œ)")

    while True:
        try:
            prompt = input("\n>>> ë‹¹ì‹ ì˜ ì—°êµ¬ ê´€ì‹¬ì‚¬/í”„ë¡¬í”„íŠ¸: ")
            if not prompt.strip(): continue

            # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            print("[System] ğŸ§  LLMì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
            # LLMì—ê²ŒëŠ” ë‹¨ì¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
            structured_kws = extract_structured_keywords_from_prompt(prompt, include_reasoning=INCLUDE_REASONING_FEATURE) 

            if structured_kws is None or not structured_kws.get('keywords'):
                print("[Warning] ìœ íš¨í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                continue

            # 2. ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ëª¨ë¸ ì…ë ¥ í…ì„œë¡œ ë³€í™˜
            prompt_kw_tensor = create_prompt_keyword_tensor(structured_kws, keyword_to_id)
            
            # 3. ì¶”ì²œ ìˆ˜í–‰
            print("[System] âœ¨ ContentAwareNet V2ê°€ ì¶”ì²œì„ ê³„ì‚°í•˜ëŠ” ì¤‘...")
            top_indices, preds = get_recommendations_for_prompt(
                model, 
                prompt_kw_tensor, 
                all_item_kws_tensor, # Item Content í…ì„œ ì „ë‹¬
                num_items,
                TOP_K
            )
            
            # 4. ê²°ê³¼ ì¶œë ¥
            print("\n" + "="*60)
            print(f"** ë¶„ì„ëœ í‚¤ì›Œë“œ (LLM) **")
            print(f"  - ì¶”ì¶œëœ í‚¤ì›Œë“œ: {structured_kws.get('keywords', [])}")
            
            if 'reasoning' in structured_kws:
                print(f"  - Reasoning: {structured_kws['reasoning']}")
                
            print(f"\n[TOP {TOP_K} ì¶”ì²œ ë…¼ë¬¸]")
            print("------------------------------------------------------------")
            
            for rank, i_new in enumerate(top_indices, 1):
                oid = reverse_item_map[i_new]
                kws_dict = node_to_structured_keywords.get(oid, {'domain': [], 'task': [], 'method': []})
                
                authors = node_to_authors.get(oid, ["Unknown Authors"])
                authors_str = ", ".join(authors[:3]) + ("..." if len(authors) > 3 else "")
                
                try:
                    pid = df_full[df_full['item_id'] == oid].iloc[0]['paper_id']
                    title = paper_title_map.get(str(pid), "Title Not Found")
                except: title = "Unknown"
                
                score = preds[i_new]
                
                print(f"[{rank}] Score: {score:.1%}")
                print(f"Title:   {title}")
                print(f"Authors: {authors_str}")
                print(f"Domain:  {kws_dict['domain']}")
                print(f"Task:    {kws_dict['task']}")
                print(f"Method:  {kws_dict['method']}")
                print("------------------------------------------------------------")

        except KeyboardInterrupt:
            print("\n\n[System] Exiting by user request. Goodbye!")
            break
        except Exception as e:
            print(f"\n[Runtime Error] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©ì„ ìœ„í•´ ì´ íŒŒì¼ì„ ì‹¤í–‰í•˜ê¸° ì „ì— 'from torch.nn.functional import cosine_similarity'ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
    main()