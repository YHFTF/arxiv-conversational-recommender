# cost_measure.py

import torch
import random
import sys
import os
import pandas as pd
import numpy as np
import json
import time


# --- í™˜ê²½ ì„¤ì • ---
FFS_LOAD_FILE = 'ogbn_arxiv_16k_ffs_sample.pt'
SAMPLE_FOR_COSTING = 100

# ğŸš¨ í•„ìˆ˜ ê²½ë¡œ ì„¤ì •
NODE_TO_ID_MAP_PATH = 'C:/Users/yungh/Desktop/data/dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv' 
TITLEABS_TSV_PATH = 'C:/Users/yungh/Desktop/data/titleabs.tsv'


# --- 1. ğŸ“‚ ì €ì¥ëœ FFS ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ ---
print("1. FFS ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ ì‹œì‘...")
# PyTorch 2.6+ ë²„ì „ì˜ ë³´ì•ˆ ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•œ íŒ¨ì¹˜ (loaderì™€ ë™ì¼)
_real_torch_load = torch.load
def _torch_load_with_weights_only_false(*args, **kwargs):
    if "weights_only" not in kwargs:
        kwargs["weights_only"] = False
    return _real_torch_load(*args, **kwargs)
torch.load = _torch_load_with_weights_only_false 

try:
    if not os.path.exists(FFS_LOAD_FILE):
        print(f"ì˜¤ë¥˜: '{FFS_LOAD_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. main_sampler.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)
        
    loaded_data = torch.load(FFS_LOAD_FILE)
    sampled_node_list = loaded_data['indices']
    print(f"âœ… FFS ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ ë…¸ë“œ ìˆ˜: {len(sampled_node_list)}ê°œ")

except Exception as e:
    print(f"â›” íŒŒì¼ ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    sys.exit(1)


# --- 2. ğŸ” ë¹„ìš© ì‚°ì •ìš© 100ê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ ---
cost_sample_indices = random.sample(sampled_node_list, SAMPLE_FOR_COSTING)
print(f"âœ… ë¹„ìš© ì‚°ì •ìš© {SAMPLE_FOR_COSTING}ê°œ ë…¸ë“œ ì¸ë±ìŠ¤ ì¶”ì¶œ ì™„ë£Œ.")


# --- 3. ğŸ—ºï¸ ë…¸ë“œ ì¸ë±ìŠ¤ -> ë…¼ë¬¸ ID ë§¤í•‘ ë¡œë“œ ---
def load_node_to_paperid_map(map_path):
    # (ì´ì „ì— ì œê³µëœ ë§µí•‘ ë¡œë“œ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    try:
        mapping_df = pd.read_csv(map_path, header=0, dtype={'idx': np.int32, 'paper id': str})
        return mapping_df['paper id'].tolist()
    except Exception as e:
        print(f"â›” ë§µí•‘ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        sys.exit(1)

node_id_list = load_node_to_paperid_map(NODE_TO_ID_MAP_PATH)
print("âœ… ë…¸ë“œ ì¸ë±ìŠ¤-ë…¼ë¬¸ ID ë§µí•‘ ë¡œë“œ ì™„ë£Œ.")


# --- 4. ğŸ“ ì´ˆë¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ (TSV ì‚¬ìš©) ---
def extract_abstracts_from_tsv(tsv_path, node_id_list, target_indices):
    # (ì´ì „ì— ì œê³µëœ TSV í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    target_paper_ids = {node_id_list[idx] for idx in target_indices}
    
    try:
        # TSV íŒŒì¼ ë¡œë“œ (í—¤ë” ì—†ìŒ ê°€ì •)
        df_texts = pd.read_csv(tsv_path, sep='\t', header=None, 
                               names=['paper id', 'title', 'abstract'], 
                               dtype={'paper id': str})
    except Exception as e:
        print(f"â›” TSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜. ê²½ë¡œ, êµ¬ë¶„ì ë˜ëŠ” ì¸ì½”ë”©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”: {e}")
        sys.exit(1)

    df_filtered = df_texts[df_texts['paper id'].isin(target_paper_ids)].set_index('paper id')

    final_abstracts = []
    for idx in target_indices:
        paper_id = node_id_list[idx]
        if paper_id in df_filtered.index:
            abstract = df_filtered.loc[paper_id, 'abstract']
            cleaned_abstract = abstract.replace("Abstract", "", 1).strip()
            final_abstracts.append(cleaned_abstract)
        else:
            final_abstracts.append("ERROR: Abstract not found in TSV.")

    return final_abstracts


abstract_texts = extract_abstracts_from_tsv(
    TITLEABS_TSV_PATH, 
    node_id_list, 
    cost_sample_indices
)

print("-" * 50)
print(f"âœ… LLM ë¹„ìš© ì‚°ì •ìš© **{len(abstract_texts)}ê°œ** ì´ˆë¡ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.")
print("--- ìƒ˜í”Œ í…ìŠ¤íŠ¸ ---")
print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ ì¸ë±ìŠ¤: {cost_sample_indices[0]}")
print(f"ì²« ë²ˆì§¸ ìƒ˜í”Œ ì´ˆë¡ (ì¼ë¶€): {abstract_texts[0][:200]}...")
print("-" * 50)
print("ğŸš€ ì´ì œ ì´ í…ìŠ¤íŠ¸ë“¤ì„ LLM APIì— ì…ë ¥í•˜ì—¬ í† í° ì‚¬ìš©ëŸ‰ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


import openai # LLM API í˜¸ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# import os # ì´ë¯¸ íŒŒì¼ ìƒë‹¨ì— ìˆìŒ

# --- ğŸš¨ LLM API ì„¤ì • ---
# 1. API í‚¤ ì„¤ì • (ì‹¤ì œ í‚¤ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤)
os.environ["OPENAI_API_KEY"] = "api í‚¤" 
client = openai.OpenAI()

# 2. ì‚¬ìš©í•  LLM ëª¨ë¸ ë° ë‹¨ê°€ ì„¤ì • (ì˜ˆì‹œ: GPT-3.5 Turbo)
LLM_MODEL = "gpt-4o-mini"
# (ë‹¨ê°€ ì˜ˆì‹œ: 1M í† í°ë‹¹ Input $0.50, Output $1.50)
INPUT_TOKEN_COST_PER_MILLION = 0.15
OUTPUT_TOKEN_COST_PER_MILLION = 0.60

# --- ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ í† í°ë§Œ ì¶”ì •í•  ê²½ìš°) ---
# ì‹¤ì œ API í˜¸ì¶œì„ ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì´ ë³€ìˆ˜ë¥¼ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
SIMULATION_MODE = False

def create_llm_prompt(abstract):
    """LLMì— ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ëª¨ë¸ì˜ ì—­í• ê³¼ ëª©í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    system_prompt = (
        "You are a Thesis Analysis AI. Read the provided abstract and extract a maximum of 10 keywords with sufficient importance that best represent the core content of the thesis as 'Features'. Additionally, provide a concise rationale, under 50 words, for predicting the thesis's topic based on these extracted keywords."
    )
    
    # ì‚¬ìš©ì ì…ë ¥: ë¶„ì„í•  ì´ˆë¡ í…ìŠ¤íŠ¸
    user_prompt = f"Thesis Abstract: \"\"\"{abstract}\"\"\""

    # ì¶œë ¥ í˜•ì‹ ìš”ì²­: JSON í˜•íƒœë¡œ ë°›ì•„ íŒŒì‹±í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
    format_prompt = (
        "The output must only be in the following JSON format: { \"features\": [\"word1\", \"word2\", ...], \"reasoning\": \"rationale text\" }"
    )

    return system_prompt + user_prompt + format_prompt


def call_llm_and_measure_tokens(abstract_text, model=LLM_MODEL, simulation=SIMULATION_MODE):
    """LLMì„ í˜¸ì¶œí•˜ê³  Input/Output í† í°ì„ ì¸¡ì •í•©ë‹ˆë‹¤."""
    
    prompt = create_llm_prompt(abstract_text)
    
    if simulation:
        # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: í† í° ìˆ˜ë¥¼ ì„ì˜ë¡œ ì¶”ì •í•©ë‹ˆë‹¤.
        # (ì‹¤ì œ í† í° ìˆ˜ë¥¼ ì–»ìœ¼ë ¤ë©´ API í˜¸ì¶œ í•„ìš”)
        input_tokens = len(prompt) // 4 + 50 # ëŒ€ëµì ì¸ í† í° ì¶”ì • (4ê¸€ìë‹¹ 1í† í° + ì˜¤ì°¨)
        output_tokens = random.randint(50, 100) # Outputì€ 50~100 í† í°ìœ¼ë¡œ ì¶”ì •
        llm_output = {"features": ["simulated", "words"], "reasoning": "This is a simulated reasoning for cost measurement."}
        
    else:
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt.split('\n')[0]}, # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
                    {"role": "user", "content": "\n".join(prompt.split('\n')[1:])}
                ],
                response_format={"type": "json_object"}
            )
            
            usage = response.usage
            input_tokens = usage.prompt_tokens
            output_tokens = usage.completion_tokens
            # LLM ì¶œë ¥ì€ JSON stringì´ë¯€ë¡œ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤.
            llm_output = json.loads(response.choices[0].message.content)

        except Exception as e:
            print(f"LLM API í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            input_tokens, output_tokens, llm_output = 0, 0, None

    return input_tokens, output_tokens, llm_output

# cost_measure.pyì˜ ê°€ì¥ ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€

# 100ê°œ ìƒ˜í”Œì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
llm_results = []
total_input_tokens = 0
total_output_tokens = 0

print("-" * 50)
print(f"3. LLM í˜¸ì¶œ ë° í† í° ì¸¡ì • ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: {len(abstract_texts)}ê°œ, ëª¨ë¸: {LLM_MODEL})")

total_start_time = time.perf_counter()
total_llm_call_duration = 0


for i, abstract_text in enumerate(abstract_texts):
    
    if "ERROR" in abstract_text:
        print(f"ê²½ê³ : ì¸ë±ìŠ¤ {cost_sample_indices[i]} í…ìŠ¤íŠ¸ ëˆ„ë½. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        continue
    print(f"ì²˜ë¦¬ ì¤‘: {i + 1}/{len(abstract_texts)}ë²ˆì§¸ ìƒ˜í”Œ (ì¸ë±ìŠ¤: {cost_sample_indices[i]})")

    call_start_time = time.perf_counter()
    input_t, output_t, output_data = call_llm_and_measure_tokens(abstract_text)
    call_end_time = time.perf_counter()
    call_duration = call_end_time - call_start_time # í•´ë‹¹ í˜¸ì¶œ ì‹œê°„
    total_llm_call_duration += call_duration # ì´ ì‹œê°„ì— ëˆ„ì 
    
    total_input_tokens += input_t
    total_output_tokens += output_t

    llm_results.append({
        'node_index': cost_sample_indices[i],
        'features': output_data['features'] if output_data else None,
        'reasoning': output_data['reasoning'] if output_data else None,
        'input_tokens': input_t,
        'output_tokens': output_t
    })
total_end_time = time.perf_counter()
overall_duration = total_end_time - total_start_time

print(f"ì´ LLM í˜¸ì¶œ ì‹¤í–‰ ì‹œê°„ (LLM API Latency): **{total_llm_call_duration:.2f}ì´ˆ**")
print(f"ì „ì²´ ë£¨í”„ ì‹¤í–‰ ì‹œê°„ (Total Runtime): **{overall_duration:.2f}ì´ˆ**")
print("-" * 50)
print("âœ… LLM í˜¸ì¶œ ë° í† í° ì¸¡ì • ì™„ë£Œ.")

# --- 4. ğŸ’° ìµœì¢… ë¹„ìš© ì‚°ì • ë° ì˜ˆì¸¡ ---

if len(abstract_texts) > 0:
    # 100ê°œ ìƒ˜í”Œì˜ í‰ê·  í† í° ì‚¬ìš©ëŸ‰
    num_processed = len(llm_results)
    avg_input_token = total_input_tokens / num_processed
    avg_output_token = total_output_tokens / num_processed

    # 1.6ë§Œ ê°œë¡œ í™•ì¥í•˜ì—¬ ì˜ˆìƒ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
    PROJECT_SIZE = 16000
    projected_input_tokens = avg_input_token * PROJECT_SIZE
    projected_output_tokens = avg_output_token * PROJECT_SIZE

    # ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (ë‹¨ìœ„: USD)
    projected_cost_input = (projected_input_tokens / 1_000_000) * INPUT_TOKEN_COST_PER_MILLION
    projected_cost_output = (projected_output_tokens / 1_000_000) * OUTPUT_TOKEN_COST_PER_MILLION
    total_projected_cost = projected_cost_input + projected_cost_output

    print("\n--- ğŸ’° LLM ë¹„ìš© ì‚°ì • ê²°ê³¼ (1.6ë§Œ ê°œ ê¸°ì¤€) ---")
    print(f"ëª¨ë¸: {LLM_MODEL}")
    print(f"ìƒ˜í”Œë‹¹ í‰ê·  Input í† í°: {avg_input_token:.2f}")
    print(f"ìƒ˜í”Œë‹¹ í‰ê·  Output í† í°: {avg_output_token:.2f}")
    print(f"ì´ ì˜ˆìƒ Input í† í° (16k): {projected_input_tokens:,.0f}ê°œ")
    print(f"ì´ ì˜ˆìƒ Output í† í° (16k): {projected_output_tokens:,.0f}ê°œ")
    print(f"ì´ ì˜ˆìƒ LLM ë¹„ìš©: **${total_projected_cost:.2f} USD** (Input: ${projected_cost_input:.2f}, Output: ${projected_cost_output:.2f})")
    print("-" * 50)

# ì¶”ì¶œëœ Featuresì™€ Reasoning ì˜ˆì‹œ í™•ì¸
print("--- ì¶”ì¶œëœ Feature/ê·¼ê±° ì˜ˆì‹œ (ì²« ë²ˆì§¸ ìƒ˜í”Œ) ---")
print(f"Features: {llm_results[0]['features']}")
print(f"Reasoning: {llm_results[0]['reasoning']}")

# (JSON íŒŒì¼ ì €ì¥ì„ ìœ„í•´ import jsonì´ í•„ìš”í•˜ë©°, osëŠ” íŒŒì¼ ê²½ë¡œ í™•ì¸ì— ì‚¬ìš©ë¨)

# --- 5. ğŸ’¾ LLM ì¶”ì¶œ ê²°ê³¼ ë°ì´í„° ì €ì¥ ---
OUTPUT_RESULTS_FILE = 'llm_costing_results.json'

print(f"\n5. ì¶”ì¶œëœ LLM ê²°ê³¼ ë°ì´í„° ì €ì¥ ì¤‘... (íŒŒì¼: {OUTPUT_RESULTS_FILE})")

try:
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(OUTPUT_RESULTS_FILE, 'w', encoding='utf-8') as f:
        # JSON íŒŒì¼ì— ì½ê¸° ì‰½ë„ë¡ ë“¤ì—¬ì“°ê¸°(indent=4)ë¥¼ ì ìš©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        json.dump(llm_results, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… LLM ì¶”ì¶œ ê²°ê³¼ ë° í† í° ì¸¡ì • ë°ì´í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_RESULTS_FILE}")
    print(f"ì´ {len(llm_results)}ê°œ ë ˆì½”ë“œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"â›” JSON íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")